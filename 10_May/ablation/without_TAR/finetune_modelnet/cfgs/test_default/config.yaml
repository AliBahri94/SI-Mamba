optimizer: {
  type: AdamW,
  kwargs: {
    lr: 0.0003,
    # lr: 0.0005,   
    weight_decay: 0.05
  } }

scheduler: {
  type: CosLR,
  kwargs: {
    epochs: 300,
    initial_epochs: 10
  } }

dataset: {
  train: { _base_: cfgs/dataset_configs/ModelNet40.yaml,
           others: { subset: 'train' } },
  val: { _base_: cfgs/dataset_configs/ModelNet40.yaml,
         others: { subset: 'test' } },
  test: { _base_: cfgs/dataset_configs/ModelNet40.yaml,
          others: { subset: 'test' } } }
model: {
  NAME: PointMamba,
  trans_dim: 384,
  depth: 12,
  cls_dim: 40,
  num_heads: 6,
  group_size: 32,
  num_group: 64,
  encoder_dims: 384, 
  rms_norm: False,
  drop_path: 0.3,
  drop_out: 0.,
  ############ config for our method  
  method: SAST,               #k_top_eigenvectors, MAMBA                              
  reverse: True,    
  reverse_2: False,      
  reverse_3: False,   
  knn_graph: 20,        
  k_top_eigenvectors: 4,                 
  alpha : 100.,          
  smallest: True,    
  symmetric : True,     
  self_loop : False,                                     
  binary : True,
  matrix : "laplacian",    
  add_after_layer : False,
  rotation : False      
}


npoints: 1024
total_bs: 32
step_per_update: 1
max_epoch: 300
grad_norm_clip: 10